# Point in Space Experiment

## Experiment Overview

This experiment views fine-tuning as vector movement in a multi-dimensional embedding space. It examines whether models can be trained to converge toward specific points (emotional targets) in this space.

## Prerequisites

1. **Environment Variables**: Ensure that the `OPENAI_API_KEY` is defined in your environment variables. This is required for interacting with the OpenAI API.
   ```bash
   export OPENAI_API_KEY=your_openai_api_key
   ```

## Running Simulations

To run simulations, use the `run_experiment.sh` script within the `single_step` and `multi_step` directories. This script handles both single-step and multi-step simulations.

Before running the script, ensure you update the `emotions` variable inside the script to specify the emotions you wish to run against.

Execute the script as follows:

```bash
./run_experiment.sh
```

## Outputs

The simulation results will be stored in the `output/` directory within both the `single_step` and `multi_step` directories. Each output file will contain the results for the specified emotions simulated during the experiment.

### Single-Step Mode

**File**:

- `all_sentences.json`

  - Contains sentences generated by the model after each round of training in single-step mode.
  - These sentences are used to evaluate how far our current model is from the emotion target centroid
  - Sentences are categorized based on `k_fraction` values:
    - `k_fraction: 33` - Model trained with 33 samples.
    - `k_fraction: 66` - Model trained with 66 samples.
    - `k_fraction: 100` - Model trained with 100 samples.

- `distance.png`:
  - Visualizes the convergence of our model as number of training samples increase.

### Multi-Step Mode

**Files**:

- `all_sentences_{iteration}.json`

  - Contains the sentences generated by the models at each iteration.
  - `sub_model_idx`: Indicates the specific model copy for the corresponding round that generated the outputs.
  - We use these files to determine the best performing model for that iteration (closest to target emotion centroid)

- `dist.png`
  - Visualizes the convergence of our model toward the target emotion centroid.
